\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi 
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{xcolor}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
% \newcommand{\titlefigure}{figure/ml-basic-riskmin-error-surface.png}
% \newcommand{\learninggoals}{\item Know the concept of loss \item Understand the relationship between loss and risk \item Understand the relationship between risk minimization and finding the best model}
\usepackage{fancy}

\colorlet{GRAY}{gray}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}
\input{../../latex-math/ml-trees.tex}
% \input{../../latex-math/ml-rf.tex}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}

%! includes: basics-learners

\lecturechapter{ML-Basics: Losses \& Risk Minimization}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{How to Evaluate Models}


 \end{vbframe}

\begin{frame}{Overview}

No Free Lunch
In machine learning, there’s something called the “No Free Lunch” theorem. In a 
nutshell, it states that no one algorithm works best for every problem, and it’s 
especially relevant for supervised learning (i.e. predictive modeling).

For example, you can’t say that neural networks are always better than decision 
trees or vice-versa. There are many factors at play, such as the size and 
structure of your dataset.

As a result, you should try many different algorithms for your problem, while 
using a hold-out “test set” of data to evaluate performance and select the 
winner.
\lz
\lz
Hypothesis space + Risk + Optimization 
\end{frame}

% ------------------------------------------------------------------------------
% CART (Classification and Regression Trees)
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{\textcolor{gray!80}{CART} ~~ Functionality}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}
% 
% \vspace{0.2cm}
% 
% \scriptsize
% 
% \colorbox{gray!80}{\textcolor{white}{SUPERVISED}} 
% \colorbox{gray!80}{\textcolor{white}{NON-PARAMETRIC}} 
% \colorbox{gray!80}{\textcolor{white}{WHITE-BOX}} 
% \colorbox{gray!80}{\textcolor{white}{FEATURE SELECTION}}
% 
% \medskip
% 
% \textbf{\textcolor{gray!80}{General idea}} {}{} Starting from a root node, 
% \textit{\textbf{classification \& regression trees (CART)}} 
% perform repeated \textbf{binary splits} of the data according to feature values, 
% thereby subsequently dividing the input space $\Xspace$ into $M$ 
% \textbf{rectangular partitions}.
% 
% \begin{itemize}
%   \item [$\rightarrow$] Pass observations along until each ends up in exactly 
%   one leaf node
%   \item [$\rightarrow$] In each step, find the optimal feature-threshold
%   combination to split by
%   \item [$\rightarrow$] Assign response $c_m$ to leaf node $m$
% \end{itemize}
% 
% \vspace{0.1cm}
% 
% \begin{minipage}{0.6\textwidth}
%   \textbf{\textcolor{gray!80}{Hypothesis space}} \\
%   $$\Hspace = \left\{ \fx: \fx = \sum_{m = 1}^M c_m \mathbb{I}(\xv \in Q_m) 
%   \right\}$$
%   \medskip
%   \textbf{\textcolor{gray!80}{Loss functions}} \\
%   Classification: mostly \textit{\textbf{Brier score, Bernoulli loss}}  \\
%   \medskip
%   Regression: mostly \textit{\textbf{quadratic loss}}
% \end{minipage}%
% \begin{minipage}{0.4\textwidth}
%   \includegraphics[width=0.8\textwidth]{figure/cart_3d.PNG}
% \end{minipage}
% 
% \medskip
% \textbf{\textcolor{gray!80}{Optimization}} {}{} Exhaustive search for optimal 
% splitting criterion (greedy optimization) \\
% \medskip
% \textbf{\textcolor{gray!80}{Hyperparameters}} {}{} Tree depth, minimum number
% of observations per node, ...
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{CART} ~~ Functionality}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\footnotesize

\colorbox{gray!80}{\textcolor{white}{SUPERVISED}} 
\colorbox{gray!80}{\textcolor{white}{NON-PARAMETRIC}} 
\colorbox{gray!80}{\textcolor{white}{WHITE-BOX}} 
\colorbox{gray!80}{\textcolor{white}{FEATURE SELECTION}}

\medskip

\textbf{\textcolor{gray!80}{General idea}} ~~ Starting from a root node, 
\textit{\textbf{classification \& regression trees (CART)}} 
perform repeated \textbf{binary splits} of the data according to feature values, 
thereby subsequently dividing the input space $\Xspace$ into $T$ 
\textbf{rectangular partitions} $Q_t$.

\medskip
 
\textbf{\textcolor{gray!80}{Hypothesis space}} ~~
$\Hspace = \left\{ \fx: \fx = \sum_{t = 1}^T c_t \I(\xv \in Q_t) 
\right\}$

\medskip

\begin{minipage}{0.5\textwidth}
  \begin{itemize}
    \item Pass observations along until each ends up in exactly 
    one leaf node
    \item In each step, find the optimal feature-threshold
    combination to \\ split by
  \item Assign response $c_t$ to leaf node $t$
\end{itemize}

\end{minipage}%
\begin{minipage}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/cart.pdf}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{CART} ~~ Functionality}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\footnotesize

\textbf{\textcolor{gray!80}{Empirical risk}} \\

\begin{itemize}
  \item Empirical risk is calculated for each potential terminal node $\Np_t$
  of a split.
  \item In general, trees can handle any type of loss function. Typical choices
  are:
  \begin{itemize}
    \footnotesize
    \item Classification (for $g$ classes):
    \begin{itemize}
      \footnotesize
      \item Using \textit{\textbf{Brier score}} ~~
      $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} \sumkg \left( \I(y = k)
      - \pikx \right)^2$
      \item Using \textit{\textbf{Bernoulli loss}} ~~
      $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} \sumkg \I(y = k) \cdot
      \log(\pikx)$
    \end{itemize}
    \item Regression: Using \textit{\textbf{quadratic loss}} ~~
    $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} (y - c_t)^2$
  \end{itemize}
\end{itemize}

\medskip

\textbf{\textcolor{gray!80}{Optimization}} ~~ \textbf{Exhaustive} search over
all (randomly selected) split candidates in each node to minimize empirical risk 
in the child nodes (greedy optimization) \\

\medskip

\textbf{\textcolor{gray!80}{Hyperparameters}} ~~ \textbf{Complexity}, i.e., 
number of leaves $T$ \\
$\rightarrow$ Controlled via tree depth, minimum number of observations per 
node, maximum number of leaves, minimum risk reduction per split, ...

\normalsize
  
\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{CART} ~~ Pro's \& Con's}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\begin{columns}[onlytextwidth]
  \begin{column}{0.5\textwidth}
    \textbf{\textcolor{gray!80}{Advantages}}
    \footnotesize
    \begin{itemize}
      \item[$\textbf{\textcolor{gray!80}{+}}$] \textbf{Easy} to understand, 
      interpret \& visualize
      \item[$\textbf{\textcolor{gray!80}{+}}$] Automatic handling of 
      \textbf{non-numerical} features
      \item[$\textbf{\textcolor{gray!80}{+}}$] Built-in \textbf{feature 
      selection}
      \item[$\textbf{\textcolor{gray!80}{+}}$] Automatic handling of 
      \textbf{missings} 
      \item[$\textbf{\textcolor{gray!80}{+}}$] \textbf{Interaction} effects 
      between features easily possible, even of higher orders
      \item[$\textbf{\textcolor{gray!80}{+}}$] \textbf{Fast} computation and 
      good scalability
      \item[$\textbf{\textcolor{gray!80}{+}}$] High \textbf{flexibility} (custom 
      split criteria or leaf-node prediction rules)   
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
    \textbf{\textcolor{gray!80}{Disadvantages}}
    \footnotesize
    \begin{itemize}
      \item[$\textbf{\textcolor{gray!80}{-}}$] Rather \textbf{low accuracy} (at 
      least, without bagging or boosting)
      \item[$\textbf{\textcolor{gray!80}{-}}$] High 
      \textbf{variance/instability}: strong dependence on training data
      \item[$\textbf{\textcolor{gray!80}{-}}$] Therefore, poor generalization \& 
      risk of \textbf{overfitting}
      \item[$\textbf{\textcolor{gray!80}{-}}$] Several steps required for
      modeling \textbf{linear} relationships
      \item[$\textbf{\textcolor{gray!80}{-}}$] In presence of categorical 
      features, \textbf{bias} towards features with \textbf{many categories}
    \end{itemize}
  \end{column}
\end{columns}

\vfill

\small

\fbox{\parbox{\textwidth}{
\centering
\textbf{Simple and good with feature selection, but not the best
predictor}}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{CART} ~~ Application}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\small

\textbf{For applications of CART, note the following:}
\lz

\textbf{\textcolor{gray!80}{Pruning / early stopping}} \\
\smallskip
Unless interrupted, splitting will go on until each leaf node contains a single 
observation (expensive + overfitting!) \\
\smallskip
$\rightarrow$ Use \textbf{pruning} and \textbf{stopping criteria} to limit 
complexity

\lz
\textbf{\textcolor{gray!80}{Implementation}} \\
\smallskip
R: package \texttt{rpart}\\
Python: \texttt{DecisionTreeClassifier} / \texttt{DecisionTreeRegressor} from 
package \texttt{scikit-learn}

\lz
\textbf{\textcolor{gray!80}{Bagging / boosting}} \\
\smallskip
Since CART are instable predictors on their own, they are typically ensembled
to form a \textit{\textbf{random forest}} (\textit{\textbf{bagging}}) or used in 
combination with \textit{\textbf{boosting}}.

\end{frame}

% ------------------------------------------------------------------------------
% RANDOM FORESTS
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{Random forests} ~~ Functionality}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\footnotesize

\colorbox{gray!80}{\textcolor{white}{SUPERVISED}} 
\colorbox{gray!80}{\textcolor{white}{NON-PARAMETRIC}} 
\colorbox{gray!80}{\textcolor{white}{BLACK-BOX}} 
\colorbox{gray!80}{\textcolor{white}{FEATURE SELECTION}}

\medskip

\textbf{\textcolor{gray!80}{General idea}} ~~ Random forests are 
\textit{\textbf{bagging ensembles}}: they combine multiple CART (weak learners)
to form a strong learner. They use \textbf{complex} trees with low  bias and 
compensate for single trees' high variance by aggregating $M$ of them in a 
\textbf{decorrelated} manner. 

\medskip

\textbf{\textcolor{gray!80}{Hypothesis space}} ~~
$\Hspace = \left\{ \fx: \fx = \frac{1}{M} \sum_{m = 1}^M \sum_{t = 1}^{T^{[m]}} 
c_t^{[m]} \I(\xv \in Q_t^{[m]}) \right\}$

\medskip

\begin{minipage}{0.5\textwidth}
  \begin{itemize}
    \item Training on bootstrap samples of the data and only on a random subset 
    of features to incur variability
    \item Aggregation via averaging (regression) or majority voting
    (classification)
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/rf_3d.PNG}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{Random forests} ~~ Functionality}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\footnotesize

\textbf{\textcolor{gray!80}{Empirical risk}} ~~
Applicable with \textbf{any} kind of loss function (just like tree base learners) 
\\
$\rightarrow$ Computation of empirical risk for all potential child nodes in all 
trees

\medskip

\textbf{\textcolor{gray!80}{Optimization}} ~~ \textbf{Exhaustive} search over
all (randomly selected) split candidates in each node of each tree to minimize
empirical risk in the child nodes (greedy optimization) \\

\medskip

\textbf{\textcolor{gray!80}{Hyperparameters}}

\begin{itemize}
  \item \textbf{Complexity}, i.e., number of leaves $T$ of each base learner \\
  $\rightarrow$ Controlled via tree depth, minimum number of observations per 
  node, maximum number of leaves, minimum risk reduction per split, ...
  \item \textbf{Ensemble size}, i.e., number of trees
  \item \textbf{Number of split candidates}, i.e., number of features to be
  considered as splitting variables at each split \\
  $\rightarrow$ Frequently used heuristics: 
  $\left \lfloor{\sqrt{p}}\right \rfloor$ for classification and
  $\left \lfloor{p/3}\right \rfloor$ for regression
  
\end{itemize}
  
\end{frame}


% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{Random forests} ~~ Pro's \& Con's}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\begin{columns}[onlytextwidth]
  \begin{column}{0.5\textwidth}
    \textbf{\textcolor{gray!80}{Advantages}}
    \footnotesize
    \begin{itemize}
      \item[$\textbf{\textcolor{gray!80}{+}}$] Translation of most advantages 
      of \textbf{trees} (e.g., inherent variable selection, handling of
      missing data)
      \item[$\textbf{\textcolor{gray!80}{+}}$] Fairly good at 
      \textbf{prediction}: improved accuracy through bagging
      \item[$\textbf{\textcolor{gray!80}{+}}$] Inherent computation of 
      \textbf{feature importance}
      \item[$\textbf{\textcolor{gray!80}{+}}$] Quite \textbf{stable} wrt changes 
      in the data
      \item[$\textbf{\textcolor{gray!80}{+}}$] Good with 
      \textbf{high-dimensional} data, even in presence of noisy covariates
      \item[$\textbf{\textcolor{gray!80}{+}}$] Applicable to \textbf{unbalanced} 
      data
      \item[$\textbf{\textcolor{gray!80}{+}}$] Easy to \textbf{parallelize}
      \item[$\textbf{\textcolor{gray!80}{+}}$] Rather easy to \textbf{tune}
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
    \textbf{\textcolor{gray!80}{Disadvantages}}
    \footnotesize
    \begin{itemize}
      \item[$\textbf{\textcolor{gray!80}{-}}$] Loss of single trees' 
      \textbf{interpretability} -- black-box method
      \item[$\textbf{\textcolor{gray!80}{-}}$] Hard to \textbf{visualize}
      \item[$\textbf{\textcolor{gray!80}{-}}$] Often suboptimal for 
      \textbf{regression}
      \item[$\textbf{\textcolor{gray!80}{-}}$] Computationally demanding --
      both in \textbf{runtime} and \textbf{memory}
      \item[$\textbf{\textcolor{gray!80}{-}}$] Often still inferior in
      \textbf{performance} to other methods (e.g., boosting)
      \item[$\textbf{\textcolor{gray!80}{-}}$] \textcolor{red}{Overfitting?}
    \end{itemize}
  \end{column}
\end{columns}

\vfill

\small

\fbox{\parbox{\textwidth}{
\centering
\textbf{Fairly good predictor, but black-box method}}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{Random forests} ~~ Application}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\textbf{For applications of CART, note the following:}
\lz

\textbf{\textcolor{gray!80}{Pruning / early stopping}} \\
\smallskip
Unless interrupted, splitting will go on until each leaf node contains a single 
observation (expensive + overfitting!) \\
\smallskip
$\rightarrow$ Use \textbf{pruning} and \textbf{stopping criteria} to limit 
complexity.

\lz
\textbf{\textcolor{gray!80}{Implementation}} \\
\smallskip
R: package \texttt{ranger}\\
Python: \texttt{RandomForestClassifier} / \texttt{RandomForestRegressor} from 
package \texttt{scikit-learn}

\end{frame}

% ------------------------------------------------------------------------------
% GRADIENT BOOSTING
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!80}{Gradient Boosting} ~~ Functionality}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!80}{\rule{\textwidth}{1pt}}

\vspace{0.3cm}

\footnotesize

\colorbox{gray!80}{\textcolor{white}{SUPERVISED}} 
\colorbox{gray!80}{\textcolor{white}{NON-PARAMETRIC}} 
\colorbox{gray!80}{\textcolor{white}{BLACK-BOX}} 

\medskip

\textbf{\textcolor{gray!80}{General idea}} ~~ Gradient boosting (GB) is an
\textit{\textbf{ensemble}} method that constructs a strong learner from weak base
learners (frequently, CART). \\
As opposed to \textit{\textbf{bagging}}, however,
it assembles the base learners in a \textbf{sequential, stage-wise} manner: in 
each iteration, GB improves the current model by adding a new component that
minimizes empirical risk. The final model is a weighted sum of base learners 
$b(\xv, \thetam)$ with weights $\betam$.

\medskip

\textbf{\textcolor{gray!80}{Hypothesis space}} ~~
$\Hspace = \left\{ \fx: \fx = \sum_{m = 1}^M \betam b(\xv, \thetam) \right\}$

\medskip

\begin{minipage}{0.5\textwidth}
  \begin{itemize}
    \item One boosting iteration $\widehat{=}$ one approximate gradient step in 
    function space, $b(\xv, \thetam)$ corresponding as closely as possible to 
    the negative loss function gradient
    \item Finding next additive component $\widehat{=}$ fitting base learner
    to current point-wise residuals
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/gb_3d.PNG}
\end{minipage}


\end{frame}

% ------------------------------------------------------------------------------

\endlecture

\end{document}








